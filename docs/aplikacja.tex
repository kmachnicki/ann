\chapter{Aplikacja}

\section{Wykorzystane technologie}

Aplikacja implementująca sieci neuronowe na potrzeby realizacji tematu oraz wykonująca badania, zrealizowana została przy wykorzystaniu języka \texttt{Python} w wersji \texttt{3.5}.

Głównym modułem, na którym oparto implementację, był \texttt{scikit-learn}. Wykorzystano go przede wszystkim do zbudowania sieci neuronowej uczoną metodą wstecznej propagacji błędu, a także skorzystano z wielu udostępnionych tam rozwiązań, jak na przykład walidacja krzyżowa i selekcja cech. Użyto modułu w wersji deweloperskiej \texttt{0.18.dev0}\footnote{http://scikit-learn.org/dev/documentation.html}, gdyż dopiero od tej wersji można tam korzystać z sieci neuronowych.

Jako implementację drugiego badanego algorytmu - \textit{Extreme Learning Machines}, posłużył moduł \texttt{Python-ELM}\footnote{https://github.com/dclambert/Python-ELM}. Wyznaczony on był na oficjalnej stronie \textit{ELM}\footnote{http://www.ntu.edu.sg/home/egbhuang/elm\_codes.html} jako jedna z bazowych implementacji tegoż algorytmu. Udostępniony kod nie był, niestety, przystosowany do działania pod wersją \textit{Pythona 3}, na potrzeby projektu przerobiono więc kod tak, aby dało się go używać.

Wszystkie wykorzystane moduły i ich przeznaczenie widnieją na poniższej tabeli:

\begin{table}[h!]
    \centering
    \caption{Wykorzystane moduły.}
    \begin{tabular}{p{3cm}p{2cm}p{11cm}}
        \toprule
        \textbf{Nazwa} & \textbf{Wersja} & \textbf{Opis} \\
        \midrule
        \texttt{scikit-learn} & 0.18 & sieć neuronowa uczona metodą wstecznej propagacji błędu, selekcja cech, walidacja krzyżowa, macierz błędu. \\
        \texttt{Python-ELM} & 0.3 & \textit{Extreme Learning Machines}. \\
        \texttt{numpy} & 1.11.0 & Obliczenia numeryczne. \\
        \texttt{matplotlib} & 1.5.1 & Tworzenie wykresów. \\
        \bottomrule
    \end{tabular}
\end{table}

\newpage

\section{Opis implementacji}

Na aplikację skłąda się kilka modułów:

\begin{itemize}
    \item \texttt{main.py} - główny moduł uruchamiający badania.
    \item \texttt{algorithms.py} - uruchamia oba algorytmy.
    \item \texttt{dataset.py} - importuje dane z pliku CSV.
    \item \texttt{grapher.py} - tworzy wykresy z wyników badań.
    \item \texttt{helper.py} - zawiera klasy pomocnicze do przechowywania danych.
    \item \texttt{consts.py} - zawiera ustawienia badań i parametry dla algorytmów.
\end{itemize}

Po uruchomieniu badania przez moduł \texttt{main.py}, w module \texttt{algorithms.py} odbywa się wykonywanie owych badań. Najpierw, na zbiorze zawierającym dane uczące \texttt{X} oraz tablicę wyników \texttt{y} wykonywana jest 10-krotna walidacja krzyżowa przy pomocy \texttt{StratifiedKFold} z modułu \texttt{sklearn.model\_selection}, w wyniku której otrzymywane są indeksy elementów podzielonych na dwa podzbiory - zbiór uczący i testowy:

\begin{verbatim}
 StratifiedKFold(n_folds=n_folds).split(X, y):
\end{verbatim}

W tym momencie następuje też iteracja przez wszystkie wyniki walidacji, gdzie w każdej iteracji odbywa się selekcja \texttt{n}-cech, gdzie \texttt{n} to liczba cech, dla której aktualnie wykonywane są badania:

\begin{verbatim}
 SelectKBest(k=k_best_features).fit(X, y).get_support(indices=True)
\end{verbatim}

Na tak wyselekcjonowanych cechach odbywa się uruchomienie dwóch algorytmów - uczonego metodą wstecznej propagacji błędu (algorytm \textit{BP}) oraz jego szybka wersja losowa \textit{Extreme Learning Machines} (algorytm \textit{ELM}). Najpierw tworzona jest instancja klasyfikatora algorytmu \textit{BP} (opis parametrów użytych do badania znajduje się w rozdziale o badaniach), następnie odbywa się jego uczenie przekazując do funkcji \texttt{fit} zbiór uczący. Na końcu dokonywane jest mierzenie testowanie algorytmu przekazując do funkcji \texttt{score} zbiór testowy.

\begin{verbatim}
 clf = MLPClassifier(...)
 clf.fit(X_train, y_train)
 score = clf.score(X_test, y_test)
\end{verbatim}

Dla algorytmu \texttt{ELM} procedura wygląda bardzo podobnie:

\begin{verbatim}
 elmc = ELMClassifier(...)
 elmc.fit(X_train, y_train)
 score = elmc.score(X_test, y_test)
\end{verbatim}

\newpage

Na końcu dla każdego algorytmu wyliczana jest macierz pomyłek, korzystając z \texttt{confusion\_matrix} z modułu \texttt{sklearn.metrics}.

\begin{verbatim}
conf_matrix = confusion_matrix(y_test, clf.predict(X_test))
conf_matrix = confusion_matrix(y_test, elmc.predict(X_test))
\end{verbatim}

\section{Wdrożenie}

W celu przygotowania aplikacji do działania, należy mając zainstalowaną wersję \texttt{Pythona 3.5} dodać katalog projektu do zmiennej środowiskowej \texttt{PYTHONPATH}. Aby tego dokonać, w katalogu projektu trzeba uruchomić komendę:

\begin{verbatim}
$ export PYTHONPATH="${PYTHONPATH}:${PWD}"
\end{verbatim}

Kolejnym krokiem jest uruchomienie skryptu instalacyjnego \texttt{install.sh}, który to zainstaluje wymagane pakiety z pliku \texttt{requirements.txt} oraz pobierze poprawiony moduł \texttt{Python-ELM} do katalogu \texttt{modules}:

\begin{verbatim}
./install.sh
\end{verbatim}

Teraz można uruchomić aplikację poprzez wywołanie skryptu głównego \texttt{main.py}:

\begin{verbatim}
python3 main.py
\end{verbatim}

Działanie aplikacji powinno się zakończyć wygenerowanymi wykresami.